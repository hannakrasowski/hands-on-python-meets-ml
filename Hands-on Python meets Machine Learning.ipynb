{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Python meets Machine Learning\n",
    "\n",
    "This python notebook gives a short introduction to python and then tackles the task of supervised learning with feed-forward fully-connected neural networks. In order to illustrate training neural networks, first a minimal example of a neural network is given to understand the mechanics of training a neural network. Second, we have a look at real computer vision task and train a neural network on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python and IPython Notebooks\n",
    "Python is a widely used general-purpose, high level programming language. It was created by Guido van Rossum in 1991 and further developed by the Python Software Foundation. \n",
    "\n",
    "An IPython notebook is made up of a number of cells. Each cell can contain Python code. You can execute a cell by clicking on it and pressing [Shift-Enter]. When you do so, the code in the cell will run, and the output of the cell will be displayed beneath the cell. For example, after running the first cell the notebook looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hello World!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables and data structures in Python\n",
    "\n",
    "In Python you do not have to declare the type the variable. Just type in the variable and when values will be given to it, then it will automatically know whether the value given would be a int, float or char or even a String.\n",
    "\n",
    "The most important data structures in Python are lists and dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to declare variables and data structures (# starts a comment in Python)\n",
    "print('First, different variables are declared:')\n",
    "myNumber = 3\n",
    "print(myNumber) \n",
    "\n",
    "myNumber2 = 4.5\n",
    "print(myNumber2) \n",
    "\n",
    "myNumber =\"helloworld\"\n",
    "print(myNumber) \n",
    "\n",
    "print('Then, different data structures:')\n",
    "myList = [1,2,3,4]\n",
    "print(myList)\n",
    "\n",
    "myDict = {'dog name': 'Bello', 'dog age':3}\n",
    "print(myDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection and Functions\n",
    "In order to make more complex code, selection and functions must be defined. \n",
    "Selection in Python is made using the two keywords ‘if’ and ‘elif’ and else (elseif). \n",
    "You can think of functions like a bunch of code that is intended to do a particular task in the whole Python script. Python used the keyword ‘def’ to define a function.\n",
    "\n",
    "In the following, we provide two examples to illustrate selection and functions. The selection code compares to values and determines the larger one. The function implemented simply adds two arguments to each other. Have a special look at the indentation, that is really important for Python and how code is structured. If you forget about the right indent, the code will not run through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to illustrate selection statement \n",
    "\n",
    "num1 = 34\n",
    "num2 = 12\n",
    "if(num1>num2): \n",
    "    print(\"num1 is greater\") \n",
    "elif(num1<num2): \n",
    "    print(\"num1 is smaller\") \n",
    "else: \n",
    "    print(\"num1 and num2 are equal\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to illustrate how functions are defined\n",
    "\n",
    "def adder(arg1, arg2): # here the name of the function and the arguments are defined\n",
    "    result = arg1 + arg2 # this is the body of the function. In this case a simple addition.\n",
    "    return result # after return the outputs are defined\n",
    "\n",
    "print('Testing adder with 1 and 5 results in:')\n",
    "result = adder(2,5) # here we apply the function for the argumens 2 and 5 and store the output in the variable result\n",
    "print(result) # print the result so we can see what it actually is\n",
    "\n",
    "print('Testing adder with 1 and -3.5 results in:')\n",
    "print(adder(1,-3.5)) # another test for the function but we print the output directly\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minitask 1**: Write a function that substracts the large value from the smaller. Name your function *substract_positive*. It recieves two arguments and returns the result of the substraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip: operators for comparison in Python: <, >, =<, >=, != (not equal)\n",
    "# CODE HERE:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(substract_positive(5,10))\n",
    "print(substract_positive(-3,4.3))\n",
    "print(substract_positive(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python packages - NumPy and Matplotlib\n",
    "For python many packages are available which already include ready to use functions, data structures or operations. You can include these packages in your Notebook in order to use them. \n",
    "\n",
    "One of the most important packages is NumPy. It is the fundamental package for scientific computing with Python. It contains among other things: a powerful N-dimensional array object; sophisticated (broadcasting) functions; useful linear algebra, Fourier transform, and random number capabilities\n",
    "\n",
    "Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. Therefore this package is often quite useful to visualize your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import numpy as np # this translates to: import package numpy and store it for usage under the name np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how to use the imported packages. Here is a short example for the relative error. It is defined as:\n",
    "$$\\frac{x-y}{x}$$ where $x$ is the real value and $y$ is the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    absolute_difference = np.abs(x - y) # np.abs() returns the absolute value so for -2 it returns 2\n",
    "    real_value = np.maximum(1e-8, np.abs(x)) # np.maximum(arg1,arg2) returns the greater argument\n",
    "    \n",
    "    return absolute_difference / real_value\n",
    "\n",
    "print(rel_error(2,2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal Example for Gradient decent\n",
    "\n",
    "The basic idea of how neural networks are trained is by minimizing a loss function. For supervised learning this loss is calculated by comparing the prediction to the actual label. The forward pass is calculating the prediction. The backward pass is calculating the gradient with respect to the parameters that lead to the prediction. This gradient is then used to update the function or network parameters in order to get closer to the optimal value. Now we will investigate a simple neural network with 3 input neurons, 4 hidden neurons and one output neuron. Therefore the network is determined by the function: <br /> \n",
    "$$ y_{predict} = a(W_2* a(W_1 * x))$$ <br /> \n",
    "where $W_2$ is a matrix for the weights between the output and the hidden layer, $W_1$ for the weights between input and hidden layer, $x$ is a sample of the data in our case three dimensional vector, and $a()$ is the non-linear activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of the Neural Network and defining the activation function\n",
    "First, we will initialize the neural net. This means we set the parameters or weights of the non-linear function to a random value. Training will then adjust the random values to meaningful ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Min_Neural_Net:\n",
    "    def __init__(self):\n",
    "            np.random.seed(10) # for generating the same results\n",
    "            self.W_1   = np.random.rand(3,4) # input to hidden layer weights\n",
    "            self.W_2   = np.random.rand(4,1) # hidden layer to output weights\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the non-linear activation function. In our case, we use the sigmoid function $\\mathrm{\\sigma}(z)$ but other functions like ReLU or tanh are also common choices for non-linear functions. For calculating the gradient, we will also need the derivative of the sigmoid function and thus we define the derivative as well. <br /> \n",
    "\n",
    "$$ \\mathrm{\\sigma}(z) = \\frac{1}{1+ \\exp^{-z}} $$\n",
    "\n",
    "$$ \\frac{\\mathrm{\\sigma}(z)}{dz} = \\mathrm{\\sigma}(z) * (1 - \\mathrm{\\sigma}(z))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, w):\n",
    "    z = np.dot(x, w)\n",
    "    return 1/(1 + np.exp(-z))\n",
    "    \n",
    "def sigmoid_derivative(x, w):\n",
    "    return sigmoid(x, w) * (1 - sigmoid(x, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "The forward pass is generating the loss which should be optimizied. Therefore the prediction with the current network weights is compared to the true values by a loss function. This can be a simple substraction (in our case) or other loss functions like the mean squared error. \n",
    "\n",
    "**Minitask 2**: Implement the forward pass for this network. The input to the function is the current neural network $NN$, the data $x$ and the labels $y_{true}$. The output should be the loss, which is in our case simply defined the difference between the true labels and the prediction ($y_{true} - y_{predict}$). Further we need to output the values at the hidden neurons for further calculation of the backward path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip: this formula describes how the prediction is caluclated: y_predict = a(W_2* a(W_1 * x))\n",
    "# Tip: the weights are stored in the neural network object NN\n",
    "# Tip: you can access the weight W_1 by writing NN.W_1\n",
    "\n",
    "def forward_pass(NN, x, y_true):\n",
    "    loss = None\n",
    "    x_hidden = None\n",
    "    # CODE HERE:\n",
    "\n",
    "    \n",
    "\n",
    "    return loss, x_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "\n",
    "The backward pass calculates the gradient with respect to the weights of the network. It looks not intuitive but in the end it is simply applying the differentiation rules like the chain rule in order to recieve the right formula. This gradient is then used to update the weights. The general update step is:\n",
    "\n",
    "$$w_{new} = w - lr * \\nabla_{w} \\mathrm{loss}(w)$$\n",
    "\n",
    "where $w$ are the weights and $lr$ is the learning rate. The learning rate is a important hyperparameter which determines how large the update step should be. A too large update step might lead to never converging to the optimal point (minimum of the loss). If the learning rate is too small training might take very long and the model mit get stuck in a small local minimum but not the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(NN, loss, x_hidden, x):\n",
    "    learning_rate = 0.1\n",
    "    \n",
    "    # gradients for hidden to output weights\n",
    "    g_w_2 = np.dot(x_hidden.T, - loss * sigmoid_derivative(x_hidden, NN.W_2))\n",
    "    # gradients for input to hidden weights\n",
    "    g_w_1 = np.dot(x.T, np.dot(- loss * sigmoid_derivative(x_hidden, NN.W_2), NN.W_2.T) * sigmoid_derivative(x, NN.W_1))\n",
    "    \n",
    "    # update weights\n",
    "    NN.W_1 -= learning_rate * g_w_1\n",
    "    NN.W_2 -= learning_rate * g_w_2\n",
    "\n",
    "    return NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now let's put everything together. So we use the forward pass for calculating the loss, the backward pass for calculating the gradient, and the gradient for updating the network weights. We will do this procedure multiple times until convergence.\n",
    "\n",
    "So have a look at a small example. First, we initialize the weights of the neural network which are the actual brain. Then we generate some data and labels for it. Here the data is $X$ and the labels are $y$. Further, we have to define how long we want to iterate. Then we do these iterations and then visualize the calculated loss. As we want to minimize the loss, the curve should decrease towards zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializations\n",
    "neural_network = Min_Neural_Net()\n",
    "print('Random starting input to hidden weights: ')\n",
    "print(neural_network.W_1)\n",
    "print('Random starting hidden to output weights: ')\n",
    "print(neural_network.W_2)\n",
    "\n",
    "# Some data to train the network on\n",
    "X = np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "y = np.array([[0, 1, 1, 0]]).T\n",
    "\n",
    "# Defining how many updates should be conducted\n",
    "iterations = 1000\n",
    "log_loss = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    loss, x_hidden = forward_pass(neural_network, X, y)\n",
    "    neural_network = backward_pass(neural_network, loss, x_hidden, X)\n",
    "    log_loss.append(np.abs(np.sum(loss)))\n",
    "    \n",
    "\n",
    "plt.plot( np.linspace(0,iterations, iterations),log_loss, label='train')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "After training the prediction is the actual value you would want from a neural network. It is the output of the network for a certain data point while the trained weights are the brain of the network. So let's have a look how well our network is performing on the data we used for training.\n",
    "\n",
    "**Minitask 3**: Write a function named *prediction* which takes the trained neural network and data as input and output the prediction vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip: the prediction is almost the same as the forward pass\n",
    "\n",
    "def prediction(NN, x):\n",
    "    y_predict = None\n",
    "    # CODE HERE:\n",
    "\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The final prediction after training for the data is:')\n",
    "print(prediction(neural_network, X))\n",
    "print('The absolute error between true values and prediction is:')\n",
    "print(y-prediction(neural_network, X))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Vision task\n",
    "\n",
    "Now we will hava a look at a larger neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset. Run the next cell to perfom the setup for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of setup\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from exercise_code.classifiers.neural_net import TwoLayerNet\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "Now that you have implemented and understood how optimization for neural networks in supervised learning works on toy data. It's time to load up the CIFAR-10 data so we can use it to train a classifier on a real dataset. First, we check how this dataset looks like to get an better impression. Further, for real machine learning we have to split the data in training, validation and test set. The training set will be used to train the neural network, the validation set is for finding the fitten hyperparameters (we come to this later), and the test set is for finnaly checking if the mnetwork we trained is performing well. Run the next cell to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.data_utils import load_CIFAR10\n",
    "from exercise_code.vis_utils import visualize_cifar10\n",
    "\n",
    "def get_CIFAR10_data(num_training=48000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'datasets/'\n",
    "    X, y = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "\n",
    "    # Our training set will be the first num_train points from the original\n",
    "    # training set.\n",
    "    mask = range(num_training)\n",
    "    X_train = X[mask]\n",
    "    y_train = y[mask]\n",
    "    \n",
    "    \n",
    "    # Our validation set will be num_validation points from the original\n",
    "    # training set.\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X[mask]\n",
    "    y_val = y[mask]\n",
    "    \n",
    "    # We use a small subset of the training set as our test set.\n",
    "    mask = range(num_training + num_validation, num_training + num_validation + num_test)\n",
    "    X_test = X[mask]\n",
    "    y_test = y[mask]\n",
    "    \n",
    "    # We will also make a development set, which is a small subset of\n",
    "    # the training set. This way the development cycle is faster.\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "\n",
    "    return X, y, X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_raw, y_raw, X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev= get_CIFAR10_data()\n",
    "print('Train data shape: {}'.format(X_train.shape))\n",
    "print('Train labels shape:{}'.format(y_train.shape))\n",
    "print('Validation data shape: {}'.format(X_val.shape))\n",
    "print('Validation labels shape:{}'.format(y_val.shape))\n",
    "print('Test data shape: {}'.format(X_test.shape))\n",
    "print('Test labels shape: {}'.format(y_test.shape))\n",
    "print('dev data shape: {}'.format(X_dev.shape))\n",
    "print('dev labels shape: {}'.format(y_dev.shape))\n",
    "\n",
    "# visualize raw data\n",
    "visualize_cifar10(X_raw, y_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Network\n",
    "As before, we will use gradient descent (in particular stochastic gradient descent (SGD)) to train our network. In addition, we will adjust the learning rate with an exponential learning rate schedule as optimization proceeds; after each epoch, we will reduce the learning rate by multiplying it by a decay rate. This is meaningful in order to allow large update steps in the beginning and smaller at the end. \n",
    "\n",
    "Run the next cell to create a Neural Network for the CIFAR-10 dataset with one hidden of 50 neurons. The input size applies to the size of the pictures - pixels $32*32$ and RGB channels $3$ - in the dataset. As we have 10 classes we want to predict the output layer has 10 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural network for the task\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets train the network for a few iterations and then check the accuracy of the prediction on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=1000, batch_size=200,\n",
    "            learning_rate=1e-4, learning_rate_decay=0.95,\n",
    "            reg=0.5, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (net.predict(X_val) == y_val).mean()\n",
    "print('Validation accuracy: {}'.format(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug the training\n",
    "With the default hyperparameters we provided above, you should get a validation accuracy of about 0.26 on the validation set. This isn't very good.\n",
    "\n",
    "One strategy for getting insight into what's wrong is to plot the loss function and the accuracies on the training and validation sets during optimization.\n",
    "\n",
    "Another strategy is to visualize the weights that were learned in the first layer of the network. In most neural networks trained on visual data, the first layer weights typically show some visible structure when visualized.\n",
    "\n",
    "To visualize the loss, accuracies and weights of the first layer, run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from exercise_code.vis_utils import visualize_grid\n",
    "\n",
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplots(nrows=2, ncols=1)\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the weights of the network\n",
    "\n",
    "def show_net_weights(net):\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    W1 = net.params['W1']\n",
    "    W1 = W1.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)\n",
    "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_net_weights(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune your hyperparameters\n",
    "\n",
    "**What's wrong?**. Looking at the visualizations above, we see that the loss is decreasing more or less linearly, which seems to suggest that the learning rate may be too low. Moreover, there is no gap between the training and validation accuracy, suggesting that the model we used has low capacity, and that we should increase its size. On the other hand, with a very large model we would expect to see more overfitting, which would manifest itself as a very large gap between the training and validation accuracy.\n",
    "\n",
    "**Tuning**. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks. You have to experiment with different values of the various hyperparameters, including hidden layer size, learning rate, numer of training epochs, and regularization strength in order to determine the best fitting values. For this exercise, we already found some good performing values. But if you want to you can also go to the file *exercise_code.classifiers.neural_net* and change the function *neuralnetwork_hyperparameter_tuning* to test out tuning your self.\n",
    "\n",
    "**Approximate results**. With this type of network, you should aim to achieve a classification accuracy of greater than 48% on the validation set. To achieve even better validation results you would have to implement further techniques like drop-out layers or batch normalization. If your interested in this have a look at:\n",
    "1. https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5 \n",
    "2. https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c\n",
    "3. https://towardsdatascience.com/how-to-increase-the-accuracy-of-a-neural-network-9f5d1c6f407d\n",
    "\n",
    "Only run the next cell if you want to tune the hyperparameters yourself, running this code might take some time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.classifiers.neural_net import TwoLayerNet, neuralnetwork_hyperparameter_tuning\n",
    "from exercise_code.model_savers import save_two_layer_net\n",
    "\n",
    "best_net, all_classifiers = neuralnetwork_hyperparameter_tuning(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# visualize the weights of the best network\n",
    "show_net_weights(best_net)\n",
    "# save_two_layer_net(best_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the trained network\n",
    "\n",
    "Run the next cell to visualize the weights of the trained network with tunes hyperparameters. \n",
    "\n",
    "**Minitask 4**: What do you observe compared to the previous visualization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.model_savers import load_two_layer_net\n",
    "\n",
    "net_stored = load_two_layer_net()\n",
    "show_net_weights(net_stored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your model\n",
    "The final step for every machine learing task is testing the trained model. This test dataset is only touched at the end of all tuning in order to ensure that you did not train your network to perform well on the test set. Then the testing shows if the model really generalizes the data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = (net_stored.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope this notebook gave you a first impression on Python, Jupyter Notebooks, and how neural networks are trained.\n",
    "There is potenially a lot more you can discover in this notebook and about machine learning and there are many online resources to learn more about Python. So in case you want to try out more your self here are some references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. https://www.geeksforgeeks.org/python-3-basics/\n",
    "2. https://numpy.org/\n",
    "3. https://matplotlib.org/\n",
    "4. https://medium.com/datathings/neural-networks-and-backpropagation-explained-in-a-simple-way-f540a3611f5e\n",
    "5. https://towardsdatascience.com/a-step-by-step-implementation-of-gradient-descent-and-backpropagation-d58bda486110\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
